{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596efa82-39c5-4378-86ef-d93ee39be3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from collections import Counter\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4a653-d384-468e-89fd-6cf158010ba3",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f7b087-bb86-4f76-abb6-2b6afe033fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Phones\\n\\nModern humans today are always on th...      0\n",
       " 1  This essay will explain if drivers should or s...      0\n",
       " 2  Driving while the use of cellular devices\\n\\nT...      0\n",
       " 3  Phones & Driving\\n\\nDrivers should not be able...      0\n",
       " 4  Cell Phone Operation While Driving\\n\\nThe abil...      0,\n",
       " 17497,\n",
       " 27371,\n",
       " 44868)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_v2_drcat_02.csv', usecols=['text','label'])\n",
    "data.head(), np.sum(data['label']==1), np.sum(data['label']==0), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7aa4e-acec-4a71-a713-3d6a4e910644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_punc(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "data['text'] = data['text'].apply(rem_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4481dcb-d0d8-429a-b332-7042f11823d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_size = 500\n",
    "dfsub = data.groupby('label').apply(lambda x: x.sample(n=subset_size, random_state=42)).reset_index(drop=True)\n",
    "len(dfsub), np.sum(dfsub['label']==1),np.sum(dfsub['label']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f0d6abf-6fd7-4d50-8c45-7817ca079958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 100, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, df_temp = train_test_split(dfsub, test_size=0.2, stratify=dfsub['label'], random_state=42)\n",
    "val, test = train_test_split(df_temp, test_size=0.5, stratify=df_temp['label'], random_state=42)\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d45ee6e2-589c-4637-a79d-a2daa991194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af073b14-7954-436c-ac30-b62936266c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = pd.read_csv('llm-detect-ai-generated-text/train_prompts.csv')\n",
    "# train = pd.read_csv('llm-detect-ai-generated-text/train_essays.csv')\n",
    "# test = pd.read_csv('llm-detect-ai-generated-text/test_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab0e74a-407b-4d70-9816-8cb459440c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts.iloc[0]['prompt_id'], prompts.iloc[0]['prompt_name'], prompts.iloc[0]['instructions'], prompts.iloc[0]['source_text']\n",
    "# prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c64452-1ce0-46ff-a7e4-38bf8fbbb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(train['generated']==0))\n",
    "# print(len(np.unique(train['id'])))\n",
    "# print(np.sum(train['prompt_id']==0))\n",
    "# print(np.sum(train['prompt_id']==1))\n",
    "# train = train.rename(columns={'generated':'label'})\n",
    "# train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9efbd98-debc-4acb-8d4d-ef269bad48b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  prompt_id          text\n",
      "0  0000aaaa          2  Aaa bbb ccc.\n",
      "1  1111bbbb          3  Bbb ccc ddd.\n",
      "2  2222cccc          4  CCC ddd eee.\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980ac6f-0487-4d70-ab89-6007c389bda5",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8694365f-38b9-4491-a01c-4fd85de408ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 100\n",
       " }))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Dataset.from_pandas(train[['text','label']])\n",
    "val = Dataset.from_pandas(val[['text','label']])\n",
    "test = Dataset.from_pandas(test[['text','label']])\n",
    "train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "368d2ed9-d467-4194-b025-e410f84c18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)  # Assuming predictions are logits; use argmax to get class indices\n",
    "    labels = np.array(labels, dtype=int)  # Ensure labels are integers\n",
    "\n",
    "    correct_predictions = np.sum(predictions == labels)\n",
    "    accuracy = correct_predictions / len(labels)\n",
    "    \n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423ec5b-9743-44f0-80f8-e4c684a2c306",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4b8060d-6e56-4601-855a-c50eab86ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'distilbert-base-uncased'\n",
    "# output_dir='who_wrote_it'\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fbcc2e2-fc8e-493a-a6d0-28fd38c2e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function1(examples):\n",
    "    return tokenizer1(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fceb14ed-f6ce-4844-9a28-820f6e0a5774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a9b8d1e76b492fb1c164cb05862bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b5b51cd64e43b89c96aab5b1e4a2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8767c425dc484baa293cf5e7b4aa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name1 = 'distilbert-base-uncased'\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "# tokens = tokenizer.tokenize(ds[0]['text'])\n",
    "train_token1 = train.map(preprocess_function1, batched=True)\n",
    "val_token1 = val.map(preprocess_function1, batched=True)\n",
    "test_token1 = test.map(preprocess_function1, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b40198a-80de-483c-b0fe-c6a9353099c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator1 = DataCollatorWithPadding(tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d31514b-0131-4e41-a8c9-c40a711c1a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name1, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca422458-a2de-4ec5-bbc0-c7b8de311786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236951</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.156122</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.35275760650634763, metrics={'train_runtime': 400.484, 'train_samples_per_second': 3.995, 'train_steps_per_second': 0.125, 'total_flos': 211947837849600.0, 'train_loss': 0.35275760650634763, 'epoch': 2.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args1 = TrainingArguments(\n",
    "    output_dir=\"who_wrote_it\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args1,\n",
    "    train_dataset=train_token1,\n",
    "    eval_dataset=val_token1,\n",
    "    tokenizer=tokenizer1,\n",
    "    data_collator=data_collator1,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a1123dc-6543-4a3e-b9d4-4b1c45f8d3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs1 = trainer1.predict(test_token1)\n",
    "# outputs1, id2label[np.argmax(outputs1.predictions[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5bccb44-21a1-4a70-a930-cd0b5e3a7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1 = outputs1.predictions\n",
    "# Apply softmax to get probabilities\n",
    "probabilities1 = np.exp(logits1) / np.sum(np.exp(logits1), axis=1, keepdims=True)\n",
    "# print(probabilities1)\n",
    "\n",
    "# The second column (index 1) corresponds to class B - ai generated text\n",
    "generated1 = probabilities1[:, 1]\n",
    "\n",
    "# print(generated1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ee2563b-d73f-4f64-9432-b900f9be2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(probs,labs):\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return np.sum(preds==labs)/len(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e92442d-9657-4c35-b074-07808205f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DistilBERT: 93.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of DistilBERT:',100*acc(probabilities1,test['label']),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf83dd-d275-4f2d-9113-e56f43baf5bd",
   "metadata": {},
   "source": [
    "### ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d21294c2-21f3-4154-9294-a594fa830a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325114f295a843a28a7375328a555c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acc8f256423494a925cae9c01ef38b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8236ea1a8c994596a3f23d3cc1a0b644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 13:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.067966</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.027002</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.21963270187377928, metrics={'train_runtime': 830.2404, 'train_samples_per_second': 1.927, 'train_steps_per_second': 0.06, 'total_flos': 38236962816000.0, 'train_loss': 0.21963270187377928, 'epoch': 2.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function2(examples):\n",
    "    return tokenizer2(examples[\"text\"], truncation=True)\n",
    "\n",
    "model_name2 = 'albert-base-v2'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "# tokens = tokenizer.tokenize(ds[0]['text'])\n",
    "train_token2 = train.map(preprocess_function2, batched=True)\n",
    "val_token2 = val.map(preprocess_function2, batched=True)\n",
    "test_token2 = test.map(preprocess_function2, batched=True)\n",
    "\n",
    "data_collator2 = DataCollatorWithPadding(tokenizer=tokenizer2)\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name2, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"who_wrote_it\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_token2,\n",
    "    eval_dataset=val_token2,\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator2,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "683202fa-ca11-4a68-9c6b-362d50c86fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs2 = trainer2.predict(test_token2)\n",
    "\n",
    "logits2 = outputs2.predictions\n",
    "# Apply softmax to get probabilities\n",
    "probabilities2 = np.exp(logits2) / np.sum(np.exp(logits2), axis=1, keepdims=True)\n",
    "# print(probabilities2)\n",
    "\n",
    "# The second column (index 1) corresponds to class B - generated by AI\n",
    "generated2 = probabilities2[:, 1]\n",
    "\n",
    "# print(generated2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5498b288-9c65-42b8-94f4-e5f64614d7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ALBERT: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of ALBERT:',100*acc(probabilities2,test['label']),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba31044-2f94-43b0-9bdc-2100a4fb4276",
   "metadata": {},
   "source": [
    "### Mean predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdb11bcb-ef07-441a-bd22-b8888e5a0a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90566695 0.10269861 0.12681969 0.9154422  0.86411685] [0.97029305 0.02136991 0.02398213 0.9716846  0.970585  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93798   , 0.06203426, 0.0754009 , 0.9435634 , 0.9173509 ,\n",
       "       0.94923806, 0.877828  , 0.95114285, 0.05777133, 0.9499601 ,\n",
       "       0.07287563, 0.9489278 , 0.8813256 , 0.91945404, 0.93826026,\n",
       "       0.07940765, 0.06568179, 0.57515156, 0.9278771 , 0.3820201 ,\n",
       "       0.9491663 , 0.06348974, 0.07036015, 0.9514024 , 0.05843329,\n",
       "       0.05698265, 0.07256859, 0.9408345 , 0.31512663, 0.38028312,\n",
       "       0.90207136, 0.9510464 , 0.08037234, 0.6948719 , 0.93919784,\n",
       "       0.94969714, 0.9275706 , 0.05721666, 0.38312206, 0.05785446,\n",
       "       0.94533485, 0.9456029 , 0.9308734 , 0.12795922, 0.9486919 ,\n",
       "       0.94227797, 0.94118786, 0.0627152 , 0.05777857, 0.9290557 ,\n",
       "       0.23128168, 0.93966514, 0.05843047, 0.8823756 , 0.9504529 ,\n",
       "       0.19577684, 0.9360187 , 0.92589974, 0.08815014, 0.31746042,\n",
       "       0.07736894, 0.0599874 , 0.05982019, 0.87717307, 0.05774756,\n",
       "       0.05591822, 0.92473865, 0.94784695, 0.8683653 , 0.9351584 ,\n",
       "       0.07020098, 0.05780216, 0.08680337, 0.05820052, 0.9447846 ,\n",
       "       0.06142196, 0.05766837, 0.06730613, 0.6353636 , 0.06135252,\n",
       "       0.4947905 , 0.06714638, 0.07290737, 0.9352815 , 0.9268373 ,\n",
       "       0.9487021 , 0.05770155, 0.95029885, 0.06300321, 0.9342009 ,\n",
       "       0.06344652, 0.05805412, 0.8152573 , 0.9144621 , 0.0568493 ,\n",
       "       0.05750321, 0.05606406, 0.936858  , 0.93611926, 0.9444065 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(generated1[:5], generated2[:5])\n",
    "preds = generated1 + generated2\n",
    "preds = preds/2\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
