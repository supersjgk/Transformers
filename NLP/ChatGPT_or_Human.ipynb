{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596efa82-39c5-4378-86ef-d93ee39be3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 19:08:51.175339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 19:08:51.763624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from collections import Counter\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4a653-d384-468e-89fd-6cf158010ba3",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f7b087-bb86-4f76-abb6-2b6afe033fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                text  label\n",
       " 0  Phones\\n\\nModern humans today are always on th...      0\n",
       " 1  This essay will explain if drivers should or s...      0\n",
       " 2  Driving while the use of cellular devices\\n\\nT...      0\n",
       " 3  Phones & Driving\\n\\nDrivers should not be able...      0\n",
       " 4  Cell Phone Operation While Driving\\n\\nThe abil...      0,\n",
       " 17497,\n",
       " 27371,\n",
       " 44868)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_v2_drcat_02.csv', usecols=['text','label'])\n",
    "data.head(), np.sum(data['label']==1), np.sum(data['label']==0), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7aa4e-acec-4a71-a713-3d6a4e910644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rem_punc(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "data['text'] = data['text'].apply(rem_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4481dcb-d0d8-429a-b332-7042f11823d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_size = 500\n",
    "dfsub = data.groupby('label').apply(lambda x: x.sample(n=subset_size, random_state=42)).reset_index(drop=True)\n",
    "len(dfsub), np.sum(dfsub['label']==1),np.sum(dfsub['label']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f0d6abf-6fd7-4d50-8c45-7817ca079958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 100, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, df_temp = train_test_split(dfsub, test_size=0.2, stratify=dfsub['label'], random_state=42)\n",
    "val, test = train_test_split(df_temp, test_size=0.5, stratify=df_temp['label'], random_state=42)\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d45ee6e2-589c-4637-a79d-a2daa991194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980ac6f-0487-4d70-ab89-6007c389bda5",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8694365f-38b9-4491-a01c-4fd85de408ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 100\n",
       " }))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Dataset.from_pandas(train[['text','label']])\n",
    "val = Dataset.from_pandas(val[['text','label']])\n",
    "test = Dataset.from_pandas(test[['text','label']])\n",
    "train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368d2ed9-d467-4194-b025-e410f84c18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)  # Assuming predictions are logits; use argmax to get class indices\n",
    "    labels = np.array(labels, dtype=int)  # Ensure labels are integers\n",
    "\n",
    "    correct_predictions = np.sum(predictions == labels)\n",
    "    accuracy = correct_predictions / len(labels)\n",
    "    \n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423ec5b-9743-44f0-80f8-e4c684a2c306",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b8060d-6e56-4601-855a-c50eab86ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'distilbert-base-uncased'\n",
    "# output_dir='chatgpt-or-human'\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# model.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fbcc2e2-fc8e-493a-a6d0-28fd38c2e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function1(examples):\n",
    "    return tokenizer1(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fceb14ed-f6ce-4844-9a28-820f6e0a5774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5188902c3f4c89bb366022209cdb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcf5d3bcd0c46ddabd968cb005b9be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cea0af8b3404ff88abe6b24a77734ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name1 = 'distilbert-base-uncased'\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
    "# tokens = tokenizer.tokenize(ds[0]['text'])\n",
    "train_token1 = train.map(preprocess_function1, batched=True)\n",
    "val_token1 = val.map(preprocess_function1, batched=True)\n",
    "test_token1 = test.map(preprocess_function1, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b40198a-80de-483c-b0fe-c6a9353099c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator1 = DataCollatorWithPadding(tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d31514b-0131-4e41-a8c9-c40a711c1a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name1, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca422458-a2de-4ec5-bbc0-c7b8de311786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.236951</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.156122</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.35275760650634763, metrics={'train_runtime': 400.484, 'train_samples_per_second': 3.995, 'train_steps_per_second': 0.125, 'total_flos': 211947837849600.0, 'train_loss': 0.35275760650634763, 'epoch': 2.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args1 = TrainingArguments(\n",
    "    output_dir=\"chatgpt-or-human\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args1,\n",
    "    train_dataset=train_token1,\n",
    "    eval_dataset=val_token1,\n",
    "    tokenizer=tokenizer1,\n",
    "    data_collator=data_collator1,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a1123dc-6543-4a3e-b9d4-4b1c45f8d3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs1 = trainer1.predict(test_token1)\n",
    "# outputs1, id2label[np.argmax(outputs1.predictions[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5bccb44-21a1-4a70-a930-cd0b5e3a7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1 = outputs1.predictions\n",
    "# Apply softmax to get probabilities\n",
    "probabilities1 = np.exp(logits1) / np.sum(np.exp(logits1), axis=1, keepdims=True)\n",
    "# print(probabilities1)\n",
    "\n",
    "# The second column (index 1) corresponds to class B - ai generated text\n",
    "generated1 = probabilities1[:, 1]\n",
    "\n",
    "# print(generated1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ee2563b-d73f-4f64-9432-b900f9be2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(probs,labs):\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return np.sum(preds==labs)/len(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e92442d-9657-4c35-b074-07808205f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DistilBERT: 93.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of DistilBERT:',100*acc(probabilities1,test['label']),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf83dd-d275-4f2d-9113-e56f43baf5bd",
   "metadata": {},
   "source": [
    "### ALBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d21294c2-21f3-4154-9294-a594fa830a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325114f295a843a28a7375328a555c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acc8f256423494a925cae9c01ef38b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8236ea1a8c994596a3f23d3cc1a0b644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 13:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.067966</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.027002</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.21963270187377928, metrics={'train_runtime': 830.2404, 'train_samples_per_second': 1.927, 'train_steps_per_second': 0.06, 'total_flos': 38236962816000.0, 'train_loss': 0.21963270187377928, 'epoch': 2.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function2(examples):\n",
    "    return tokenizer2(examples[\"text\"], truncation=True)\n",
    "\n",
    "model_name2 = 'albert-base-v2'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
    "# tokens = tokenizer.tokenize(ds[0]['text'])\n",
    "train_token2 = train.map(preprocess_function2, batched=True)\n",
    "val_token2 = val.map(preprocess_function2, batched=True)\n",
    "test_token2 = test.map(preprocess_function2, batched=True)\n",
    "\n",
    "data_collator2 = DataCollatorWithPadding(tokenizer=tokenizer2)\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name2, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"chatgpt-or-human\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_token2,\n",
    "    eval_dataset=val_token2,\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator2,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10e4986f-49cb-4a29-a10a-3726efa0dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('who_wrote_it/tokenizer_config.json',\n",
       " 'who_wrote_it/special_tokens_map.json',\n",
       " 'who_wrote_it/spiece.model',\n",
       " 'who_wrote_it/added_tokens.json',\n",
       " 'who_wrote_it/tokenizer.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir='chatgpt-or-human'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name2)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "683202fa-ca11-4a68-9c6b-362d50c86fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs2 = trainer2.predict(test_token2)\n",
    "\n",
    "logits2 = outputs2.predictions\n",
    "# Apply softmax to get probabilities\n",
    "probabilities2 = np.exp(logits2) / np.sum(np.exp(logits2), axis=1, keepdims=True)\n",
    "# print(probabilities2)\n",
    "\n",
    "# The second column (index 1) corresponds to class B - generated by AI\n",
    "generated2 = probabilities2[:, 1]\n",
    "\n",
    "# print(generated2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5498b288-9c65-42b8-94f4-e5f64614d7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ALBERT: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of ALBERT:',100*acc(probabilities2,test['label']),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba31044-2f94-43b0-9bdc-2100a4fb4276",
   "metadata": {},
   "source": [
    "### Mean predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdb11bcb-ef07-441a-bd22-b8888e5a0a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90566695 0.10269861 0.12681969 0.9154422  0.86411685] [0.97029305 0.02136991 0.02398213 0.9716846  0.970585  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93798   , 0.06203426, 0.0754009 , 0.9435634 , 0.9173509 ,\n",
       "       0.94923806, 0.877828  , 0.95114285, 0.05777133, 0.9499601 ,\n",
       "       0.07287563, 0.9489278 , 0.8813256 , 0.91945404, 0.93826026,\n",
       "       0.07940765, 0.06568179, 0.57515156, 0.9278771 , 0.3820201 ,\n",
       "       0.9491663 , 0.06348974, 0.07036015, 0.9514024 , 0.05843329,\n",
       "       0.05698265, 0.07256859, 0.9408345 , 0.31512663, 0.38028312,\n",
       "       0.90207136, 0.9510464 , 0.08037234, 0.6948719 , 0.93919784,\n",
       "       0.94969714, 0.9275706 , 0.05721666, 0.38312206, 0.05785446,\n",
       "       0.94533485, 0.9456029 , 0.9308734 , 0.12795922, 0.9486919 ,\n",
       "       0.94227797, 0.94118786, 0.0627152 , 0.05777857, 0.9290557 ,\n",
       "       0.23128168, 0.93966514, 0.05843047, 0.8823756 , 0.9504529 ,\n",
       "       0.19577684, 0.9360187 , 0.92589974, 0.08815014, 0.31746042,\n",
       "       0.07736894, 0.0599874 , 0.05982019, 0.87717307, 0.05774756,\n",
       "       0.05591822, 0.92473865, 0.94784695, 0.8683653 , 0.9351584 ,\n",
       "       0.07020098, 0.05780216, 0.08680337, 0.05820052, 0.9447846 ,\n",
       "       0.06142196, 0.05766837, 0.06730613, 0.6353636 , 0.06135252,\n",
       "       0.4947905 , 0.06714638, 0.07290737, 0.9352815 , 0.9268373 ,\n",
       "       0.9487021 , 0.05770155, 0.95029885, 0.06300321, 0.9342009 ,\n",
       "       0.06344652, 0.05805412, 0.8152573 , 0.9144621 , 0.0568493 ,\n",
       "       0.05750321, 0.05606406, 0.936858  , 0.93611926, 0.9444065 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(generated1[:5], generated2[:5])\n",
    "preds = generated1 + generated2\n",
    "preds = preds/2\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef671b-58b1-41f9-9ebf-113e55e21419",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d3fc589-8365-4b83-8df7-0b85ff68719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "\u001b[K     |████████████████████████████████| 413 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./myenv/lib/python3.8/site-packages (from optuna) (1.24.3)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.8/site-packages (from optuna) (4.66.1)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
      "\u001b[K     |████████████████████████████████| 230 kB 61.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in ./myenv/lib/python3.8/site-packages (from optuna) (6.0.1)\n",
      "Collecting sqlalchemy>=1.3.0\n",
      "  Downloading SQLAlchemy-2.0.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 66.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorlog\n",
      "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.8/site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4 in ./myenv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 18.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in ./myenv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in ./myenv/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (6.8.0)\n",
      "Collecting greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))\n",
      "  Downloading greenlet-3.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (664 kB)\n",
      "\u001b[K     |████████████████████████████████| 664 kB 126.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in ./myenv/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./myenv/lib/python3.8/site-packages (from importlib-resources; python_version < \"3.9\"->alembic>=1.5.0->optuna) (3.17.0)\n",
      "Installing collected packages: greenlet, sqlalchemy, Mako, alembic, colorlog, optuna\n",
      "Successfully installed Mako-1.3.0 alembic-1.13.0 colorlog-6.8.0 greenlet-3.0.2 optuna-3.5.0 sqlalchemy-2.0.23\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3916cd6f-6bc0-4c4a-907e-293758738c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 19:50:03,794] A new study created in memory with name: no-name-e877dcca-841f-4938-a0c5-b1531bdc2be2\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.066419</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.051831</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 19:56:49,433] Trial 0 finished with value: 0.99 and parameters: {'learning_rate': 5.113189892146903e-05, 'batch_size': 16}. Best is trial 0 with value: 0.99.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>0.043671</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.219415</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 20:03:36,208] Trial 1 finished with value: 0.95 and parameters: {'learning_rate': 9.935729571831524e-05, 'batch_size': 16}. Best is trial 0 with value: 0.99.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 5.113189892146903e-05, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', 5e-5, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Define Trainer arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='chatgpt-or-human',\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        evaluation_strategy='epoch',\n",
    "        logging_dir='./logs',\n",
    "        logging_strategy='epoch'\n",
    "    )\n",
    "\n",
    "    # Define Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_token1,\n",
    "        eval_dataset=val_token1,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_accuracy\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    predictions = trainer.predict(trainer.eval_dataset)\n",
    "    y_true = predictions.label_ids\n",
    "    y_pred = predictions.predictions.argmax(axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "459c79d1-091c-4a82-a2f5-e1fd0c78c1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of best hyperparameters using Optuna on DistilBERT: {'eval_loss': 0.7063475251197815, 'eval_accuracy': 0.5, 'eval_runtime': 7.6745, 'eval_samples_per_second': 13.03, 'eval_steps_per_second': 0.521}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define Trainer arguments for the best hyperparameters\n",
    "best_training_args = TrainingArguments(\n",
    "    output_dir='chatgpt-or-human',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=best_params['batch_size'],\n",
    "    per_device_eval_batch_size=best_params['batch_size'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=best_training_args,\n",
    "    train_dataset=train_token1,\n",
    "    eval_dataset=test_token1,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_accuracy\n",
    ")\n",
    "\n",
    "# Evaluate the model on the new test set\n",
    "results = trainer.evaluate()\n",
    "print(\"Results of best hyperparameters using Optuna on DistilBERT:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33878819-c29f-458f-994a-f8279d382e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DistilBERT optimized with Optuna: 67.0 %\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(test_token1)\n",
    "\n",
    "logits = outputs.predictions\n",
    "\n",
    "probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "# generated2 = probabilities2[:, 1]\n",
    "print('Accuracy of DistilBERT optimized with Optuna:',100*acc(probabilities,test['label']),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71389fa9-13d7-4a61-b914-2e5fee9aefab",
   "metadata": {},
   "source": [
    "#### Increase hyper parameter search space and n_trials to get better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
